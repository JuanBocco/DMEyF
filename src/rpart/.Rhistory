require("data.table")
require("rpart")
require("rpart.plot")
dataset  <- fread("./datasets/competencia1_2022.csv")
setwd("~/Desktop/Maestría/DM_EyF")#Establezco el Working Directory
getwd()
#cargo el dataset
dataset  <- fread("./datasets/competencia1_2022.csv")
dtrain  <- dataset[ foto_mes==202101 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202103 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.3,   #esto significa no limitar la complejidad de los splits
minsplit=  0,     #minima cantidad de registros para que se haga el split
minbucket= 1,     #tamaño minimo de una hoja
maxdepth=  10 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_001.csv",
sep=  "," )
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.3,   #esto significa no limitar la complejidad de los splits
minsplit=  0,     #minima cantidad de registros para que se haga el split
minbucket= 1,     #tamaño minimo de una hoja
maxdepth=  17 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_001.csv",
sep=  "," )
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("~/Desktop/Maestría/DM_EyF")#Establezco el Working Directory
getwd()
#cargo el dataset
dataset  <- fread("./datasets/competencia1_2022.csv")
dtrain  <- dataset[ foto_mes==202101 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202103 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.3,   #esto significa no limitar la complejidad de los splits
minsplit=  0,     #minima cantidad de registros para que se haga el split
minbucket= 1,     #tamaño minimo de una hoja
maxdepth=  17 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_001.csv",
sep=  "," )
# Librerías necesarias
require("data.table")
require("rpart")
require("ggplot2")
setwd("~/Desktop/Maestría/DM_EyF"
# Poner la carpeta de la materia de SU computadora local
setwd("~/Desktop/Maestría/DM_EyF")
semillas <- c(17, 19, 23, 29, 31)
dataset <- fread("./datasets/competencia1_2022.csv")
dtrain <- dataset[foto_mes == 202101]
# Generamos el primer modelo
arbol <- rpart(formula =    "clase_ternaria ~ .",
data =      dtrain,
xval =      0,
cp =       -0.3,
minsplit =  0,
minbucket = 1,
maxdepth =  4)
View(arbol)
# La siguiente función devuelve todas las hojas (nodos terminales) en una tabla
# para poder analizar mejor nuestro árbol.
tablahojas <- function(arbol, datos, target = "clase_ternaria") {
# Tomamos la columna con el target
target_vector <- datos[, get(target)]
# Tomamos las clases de nuestro target
classes <- unique(target_vector)
# Tomamos las posicion de las hojas que aplican a los registro de nuestro ds
row_leaf <- unique(arbol$where)
leaves <- data.table(row_frame = row_leaf)
setkey(leaves,row_frame)
# Relacion target ~ hojas
leaves_target <- dcast(
data.table(
target = target_vector,
leaf = arbol$where),
leaf ~ target, length,
value.var = "target")
setkey(leaves_target, leaf)
# Juntamos todo
leaves_target <- leaves_target[leaves, nomatch = 0]
# Sumamos algunas columnas calculadas
colnames(leaves_target[, classes, with = FALSE])[apply(
leaves_target[, classes, with = FALSE], 1, which.max)]
# Clase mayoritaria
leaves_target[, y := colnames(
leaves_target[, classes, with = FALSE]
)[apply(leaves_target[, classes, with = FALSE],
1, which.max)]]
# Cantidad de elementos de la hoja
leaves_target[, TOTAL := unlist(Reduce(function(a, b) Map(`+`, a, b), .SD)),
.SDcols = classes]
leaves_target
}
# Ejecutamos la función sobre nuestro modelo, con nuestros datos
hojas <- tablahojas(arbol, dtrain)
View(hojas)
## Preguntas
## - ¿Con qué criterio eligió la clase de cada hoja que determino la
##   clasificación de los registros?
## - ¿Cuántas hojas con BAJAS+2 hay?
hojas[, ganancia := `BAJA+2` * 78000 - 2000 * (CONTINUA + `BAJA+1`)]
View(hojas)
View(hojas[ganancia > 0, .(
ganancia = sum(ganancia),
enviados = sum(TOTAL),
sevan = sum(`BAJA+2`))])
## Preguntas
# Creamos un nuevo target binario
dtrain[, clase_binaria := ifelse(
clase_ternaria == "BAJA+2",
"evento",
"noevento"
)]
# Borramos el target viejo
dtrain[, clase_ternaria := NULL]
arbolbinario <- rpart("clase_binaria ~ .",
data =      dtrain,
xval =      0,
cp =       -0.3,
minsplit =  0,
minbucket = 5,
maxdepth =  4)
# Transformamos las hojas a una tabla
hojasbinario <- tablahojas(arbolbinario, dtrain, "clase_binaria")
# Y agregamos la ganancia de cada hoja
hojasbinario[, ganancia := evento * 78000 - 2000 * noevento]
View(hojasbinario)
# Por último sumarizamos
View(hojasbinario[ganancia > 0,
.(ganancia = sum(ganancia), enviados = sum(TOTAL), sevan = sum(evento))])
## Pregunta
## - ¿Considera que la agrupación de clases fue positiva para la  ganancia?
View(hojas[ganancia > 0, .(
ganancia = sum(ganancia),
enviados = sum(TOTAL),
sevan = sum(`BAJA+2`))])
dtrain[, clase_binaria := ifelse(
clase_ternaria == "BAJA+2",
"evento",
"noevento"
)]
dtrain[, clase_binaria := ifelse(
clase_ternaria == "BAJA+2",
"evento",
"noevento"
)]
semillas <- c(17, 19, 23, 29, 31)
# Cargamos el dataset
dataset <- fread("./datasets/competencia1_2022.csv")
dtrain <- dataset[foto_mes == 202101]
# Generamos el primer modelo
arbol <- rpart(formula =    "clase_ternaria ~ .",
data =      dtrain,
xval =      0,
cp =       -0.3,
minsplit =  0,
minbucket = 1,
maxdepth =  4)
View(arbol)
tablahojas <- function(arbol, datos, target = "clase_ternaria") {
# Tomamos la columna con el target
target_vector <- datos[, get(target)]
# Tomamos las clases de nuestro target
classes <- unique(target_vector)
# Tomamos las posicion de las hojas que aplican a los registro de nuestro ds
row_leaf <- unique(arbol$where)
leaves <- data.table(row_frame = row_leaf)
setkey(leaves,row_frame)
# Relacion target ~ hojas
leaves_target <- dcast(
data.table(
target = target_vector,
leaf = arbol$where),
leaf ~ target, length,
value.var = "target")
setkey(leaves_target, leaf)
# Juntamos todo
leaves_target <- leaves_target[leaves, nomatch = 0]
# Sumamos algunas columnas calculadas
colnames(leaves_target[, classes, with = FALSE])[apply(
leaves_target[, classes, with = FALSE], 1, which.max)]
# Clase mayoritaria
leaves_target[, y := colnames(
leaves_target[, classes, with = FALSE]
)[apply(leaves_target[, classes, with = FALSE],
1, which.max)]]
# Cantidad de elementos de la hoja
leaves_target[, TOTAL := unlist(Reduce(function(a, b) Map(`+`, a, b), .SD)),
.SDcols = classes]
leaves_target
}
hojas <- tablahojas(arbol, dtrain)
View(hojas)
hojas[, ganancia := `BAJA+2` * 78000 - 2000 * (CONTINUA + `BAJA+1`)]
View(hojas)
View(hojas[ganancia > 0, .(
ganancia = sum(ganancia),
enviados = sum(TOTAL),
sevan = sum(`BAJA+2`))])
dtrain[, clase_binaria := ifelse(
clase_ternaria == "BAJA+2",
"evento",
"noevento"
)]
# Borramos el target viejo
dtrain[, clase_ternaria := NULL]
arbolbinario <- rpart("clase_binaria ~ .",
data =      dtrain,
xval =      0,
cp =       -0.3,
minsplit =  0,
minbucket = 5,
maxdepth =  4)
# Transformamos las hojas a una tabla
hojasbinario <- tablahojas(arbolbinario, dtrain, "clase_binaria")
# Y agregamos la ganancia de cada hoja
hojasbinario[, ganancia := evento * 78000 - 2000 * noevento]
View(hojasbinario)
# Por último sumarizamos
View(hojasbinario[ganancia > 0,
.(ganancia = sum(ganancia), enviados = sum(TOTAL), sevan = sum(evento))])
dtrain[, clase_binaria := ifelse(
clase_ternaria == "BAJA+2",
"evento",
"noevento"
)]
semillas <- c(17, 19, 23, 29, 31)
# Cargamos el dataset
dataset <- fread("./datasets/competencia1_2022.csv")
dtrain <- dataset[foto_mes == 202101]
# Generamos el primer modelo
arbol <- rpart(formula =    "clase_ternaria ~ .",
data =      dtrain,
xval =      0,
cp =       -0.3,
minsplit =  0,
minbucket = 1,
maxdepth =  4)
View(arbol)
## Preguntas
## Usualmente se suele cortar las variables en 2 intervalos
## - ¿Se podría cortar en más intervalos?
## - ¿Cuál sería el costo?
## - ¿Se obtendrían mejores resultados?
##
## Una de las muchas ventajas que tienen los árboles es la simpleza que tienen
## para ser implementados en fácilmente en sistemas productivos, dado que la
## reescritura de las reglas de salida es muy simple.
## Step 2: De árbol a tabla
## ---------------------------
# La siguiente función devuelve todas las hojas (nodos terminales) en una tabla
# para poder analizar mejor nuestro árbol.
tablahojas <- function(arbol, datos, target = "clase_ternaria") {
# Tomamos la columna con el target
target_vector <- datos[, get(target)]
# Tomamos las clases de nuestro target
classes <- unique(target_vector)
# Tomamos las posicion de las hojas que aplican a los registro de nuestro ds
row_leaf <- unique(arbol$where)
leaves <- data.table(row_frame = row_leaf)
setkey(leaves,row_frame)
# Relacion target ~ hojas
leaves_target <- dcast(
data.table(
target = target_vector,
leaf = arbol$where),
leaf ~ target, length,
value.var = "target")
setkey(leaves_target, leaf)
# Juntamos todo
leaves_target <- leaves_target[leaves, nomatch = 0]
# Sumamos algunas columnas calculadas
colnames(leaves_target[, classes, with = FALSE])[apply(
leaves_target[, classes, with = FALSE], 1, which.max)]
# Clase mayoritaria
leaves_target[, y := colnames(
leaves_target[, classes, with = FALSE]
)[apply(leaves_target[, classes, with = FALSE],
1, which.max)]]
# Cantidad de elementos de la hoja
leaves_target[, TOTAL := unlist(Reduce(function(a, b) Map(`+`, a, b), .SD)),
.SDcols = classes]
leaves_target
}
# Ejecutamos la función sobre nuestro modelo, con nuestros datos
hojas <- tablahojas(arbol, dtrain)
View(hojas)
## Preguntas
## - ¿Con qué criterio eligió la clase de cada hoja que determino la
##   clasificación de los registros?
## - ¿Cuántas hojas con BAJAS+2 hay?
## Step 3: Calculando la ganancia de cada hoja
## ---------------------------
# Agregamos un nuevo campo de nombre ganancia
hojas[, ganancia := `BAJA+2` * 78000 - 2000 * (CONTINUA + `BAJA+1`)]
View(hojas)
## Pregunta
## - ¿Cuantás hojas que no son BAJA+2 tienen aún así ganancia positiva?
## Step 4: Sumarizando el envío
## ---------------------------
View(hojas[ganancia > 0, .(
ganancia = sum(ganancia),
enviados = sum(TOTAL),
sevan = sum(`BAJA+2`))])
## Preguntas
## Si enviaramos todos los casos de las hojas con ganancia positiva
## - ¿Cuánta ganancia tendríamos?
## - ¿Cuánta personas estimularíamos?
## - ¿A cuántas personas acertaríamos?
## Step 5: Binarizando la salida (en tu cara RAE)
## ---------------------------
# Creamos un nuevo target binario
dtrain[, clase_binaria := ifelse(
clase_ternaria == "BAJA+2",
"evento",
"noevento"
)]
# Borramos el target viejo
dtrain[, clase_ternaria := NULL]
arbolbinario <- rpart("clase_binaria ~ .",
data =      dtrain,
xval =      0,
cp =       -0.3,
minsplit =  0,
minbucket = 1,
maxdepth =  4)
# Transformamos las hojas a una tabla
hojasbinario <- tablahojas(arbolbinario, dtrain, "clase_binaria")
# Y agregamos la ganancia de cada hoja
hojasbinario[, ganancia := evento * 78000 - 2000 * noevento]
View(hojasbinario)
# Por último sumarizamos
View(hojasbinario[ganancia > 0,
.(ganancia = sum(ganancia), enviados = sum(TOTAL), sevan = sum(evento))])
## Pregunta
## - ¿Considera que la agrupación de clases fue positiva para la  ganancia?
View(hojasbinario[ganancia > 0,
.(ganancia = sum(ganancia), enviados = sum(TOTAL), sevan = sum(evento))])
# Calculamos la probabilidad de evento en cada hoja
hojasbinario[, p_evento := evento / (evento + noevento)]
# Ordenamos de forma descendiente las probabilidades, ya que nos interesan
# ante todo las probabilidades más altas
hojasordenadas <- hojasbinario[order(-p_evento),]
# Calculamos la ganancia acumulada, desde con la probabilidad desde la primera
# fila con probabilidad más alta hasta la fila N, para cada fila.
hojasordenadas[, gan_acum := cumsum(ganancia)]
View(hojasordenadas)
arbolbinario <- rpart("clase_binaria ~ .",
data =      dtrain,
xval =      0,
cp =       -0.3,
minsplit =  0,
minbucket = 5,
maxdepth =  4)
# Transformamos las hojas a una tabla
hojasbinario <- tablahojas(arbolbinario, dtrain, "clase_binaria")
# Y agregamos la ganancia de cada hoja
hojasbinario[, ganancia := evento * 78000 - 2000 * noevento]
View(hojasbinario)
# Por último sumarizamos
View(hojasbinario[ganancia > 0,
.(ganancia = sum(ganancia), enviados = sum(TOTAL), sevan = sum(evento))])
## Pregunta
## - ¿Considera que la agrupación de clases fue positiva para la  ganancia?
## Step 6: Salida probabilísticas
## ---------------------------
# Calculamos la probabilidad de evento en cada hoja
hojasbinario[, p_evento := evento / (evento + noevento)]
# Ordenamos de forma descendiente las probabilidades, ya que nos interesan
# ante todo las probabilidades más altas
hojasordenadas <- hojasbinario[order(-p_evento),]
# Calculamos la ganancia acumulada, desde con la probabilidad desde la primera
# fila con probabilidad más alta hasta la fila N, para cada fila.
hojasordenadas[, gan_acum := cumsum(ganancia)]
View(hojasordenadas)
# TAREAS:
# - Calculé la probabilidad de NO evento
# - Puede pasar que dos hojas tengan la misma probabilidad, escriba una query
#   que las agrupe.
## Preguntas
## - ¿Cómo ve la relación entre la probabilidad ordenada y la hojas con
##   ganancia?
## - ¿Cuál es la máxima ganancia posible es nuestro árbol?
## - ¿Cuál es el `punto de corte` que sugiere?
## - ¿Por qué es distinto al teórico?
## - ¿Es nuestro `punto de corte` es igual de útil?
ggplot(hojasordenadas, aes(x = p_evento ,y = gan_acum)) +
scale_x_reverse() +
geom_line(size = 1)
hojasordenadas[, c("evento_acum","noevento_acum") :=
list(cumsum(evento),cumsum(noevento))]
total_evento <- hojasordenadas[, sum(evento)]
total_noevento <- hojasordenadas[, sum(noevento)]
hojasordenadas[, c("evento_restantes", "noevento_restantes") :=
list(total_evento - evento_acum, total_noevento - noevento_acum)]
hojasordenadas[, tp := evento_acum]
hojasordenadas[, tn := noevento_restantes]
hojasordenadas[, fp := noevento_acum]
hojasordenadas[, fn := evento_restantes]
# Para validar los cálculos anteriores vamos a visualizar solo los campos
# importantes
View(hojasordenadas[, .(p_evento, evento, noevento, tp, tn, fp, fn)])
# Calculamos las variables necesarios para la curva ROC
hojasordenadas[, tpr := (tp / (tp + fn))]
hojasordenadas[, fpr := (fp / (fp + tn))]
# La graficamos
ggplot(hojasordenadas, aes(x = fpr, y = tpr)) +
# Agregamos la función identidad
geom_abline(intercept = 0, slope = 1) +
geom_line(lwd = 1)
# install.packages("geometry")
require("geometry")
x <- c(hojasordenadas$fpr,1)
y <- c(hojasordenadas$tpr, 0)
# El valor de la auc
View(polyarea(x, y))
# Calculamos su área, necesita instalar el siguiente paquete
install.packages("geometry")
require("geometry")
x <- c(hojasordenadas$fpr,1)
y <- c(hojasordenadas$tpr, 0)
# El valor de la auc
View(polyarea(x, y))
# Podemos construir una curva para el accuraccy
hojasordenadas[, acc := ((tp + tn) / (tp + tn + fp + fn))]
# Y graficarla
ggplot(hojasordenadas, aes(x = p_evento, y = acc)) +
geom_line(lwd = 1)
